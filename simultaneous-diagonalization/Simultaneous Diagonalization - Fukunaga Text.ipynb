{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fukunaga (1990), *Introduction to Statistical Pattern Recognition (2nd ed.)*\n",
    "***\n",
    "\n",
    "The Fukunaga text considers simultaneous diagonalization in multiple multivariate statistics problems. First, we begin with their simultaneous diagonalization method.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this document we consider the problem of diagonalizing $k$ matrices of dimensions $n \\times n$. That is, let $\\textbf{A}_1, \\ldots, \\textbf{A}_k \\in \\mathbb{R}_{n \\times n}$. We say that $\\textbf{Q}$ *simultaneously diagonalizes* $\\textbf{A}_1, \\ldots, \\textbf{A}_k$ if\n",
    "\n",
    "$$\\textbf{Q}'\\textbf{A}_k \\textbf{Q} = \\textbf{D}_k,$$\n",
    "\n",
    "where $\\textbf{D}_k$ is diagonal for all $k$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorem (p. 31)\n",
    "\n",
    "Let $\\boldsymbol{\\Sigma}_1$ and $\\boldsymbol{\\Sigma}_2$ be $p \\times p$ symmetric matrices (covariance matrices?).\n",
    "\n",
    "(1) First, we whiten $\\boldsymbol{\\Sigma}_1$ by\n",
    "\n",
    "$$\n",
    "\\textbf{Y} = \\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Phi}' \\textbf{X},\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\Theta}$ and $\\boldsymbol{\\Phi}$ are the eigenvalues and eigenvector matrices of $\\boldsymbol{\\Sigma}_1$, respectively, as\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Sigma}_1 \\boldsymbol{\\Phi} = \\boldsymbol{\\Phi} \\boldsymbol{\\Theta} \\quad \\text{and} \\quad \\boldsymbol{\\Phi}'\\boldsymbol{\\Phi} = \\textbf{I}_p.\n",
    "$$\n",
    "\n",
    "Then, $\\boldsymbol{\\Sigma}_1$ and $\\boldsymbol{\\Sigma}_2$ are transformed to\n",
    "\n",
    "\\begin{align}\n",
    "    \\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Phi}' \\boldsymbol{\\Sigma}_1 \\boldsymbol{\\Phi} \\boldsymbol{\\Theta}^{-1/2} &= \\textbf{I}_p \\\\\n",
    "    \\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Phi}' \\boldsymbol{\\Sigma}_2 \\boldsymbol{\\Phi} \\boldsymbol{\\Theta}^{-1/2} &= \\textbf{K}.\n",
    "\\end{align}\n",
    "\n",
    "In general, $\\textbf{K}$ is not a diagonal matrix.\n",
    "\n",
    "(2) Second, we apply the orthonormal transformation to diagonalize $\\textbf{K}$. That is,\n",
    "\n",
    "$$\n",
    "\\textbf{Z} = \\boldsymbol{\\Psi}' \\textbf{Y},\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\Psi}$ and $\\boldsymbol{\\Lambda}$ are the eigenvector and eigenvalue matrices of $\\textbf{K}$ as\n",
    "\n",
    "$$\n",
    "\\textbf{K} \\boldsymbol{\\Psi} = \\boldsymbol{\\Psi} \\boldsymbol{\\Lambda} \\quad \\text{and} \\quad \\boldsymbol{\\Psi}'\\boldsymbol{\\Psi} = \\textbf{I}_p.\n",
    "$$\n",
    "\n",
    "Equation 2.92 states that a covariance matrix is invariant under any orthonormal transformation after a whitening transformation. Hence, the whitened $\\boldsymbol{\\Sigma}_1$, i.e., $\\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Phi}' \\boldsymbol{\\Sigma}_1 \\boldsymbol{\\Phi} \\boldsymbol{\\Theta}^{-1/2}$, is invariant under the transformation $\\boldsymbol{\\Psi}$. Thus,\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\Psi}'\\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Phi}' \\boldsymbol{\\Sigma}_1 \\boldsymbol{\\Phi} \\boldsymbol{\\Theta}^{-1/2}\\boldsymbol{\\Psi} &= \\boldsymbol{\\Psi}'\\boldsymbol{\\Psi} = \\textbf{I}_p, \\\\\n",
    "\\boldsymbol{\\Psi}' \\textbf{K} \\boldsymbol{\\Psi} &= \\boldsymbol{\\Lambda}.\n",
    "\\end{align}\n",
    "\n",
    "Thus, both matrices, $\\boldsymbol{\\Sigma}_1$ and $\\boldsymbol{\\Sigma}_2$, are diagonalized. The combination of steps (1) and (2) gives the overall transformation matrix $\\boldsymbol{\\Phi} \\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Psi}$. The following figure shows a 2-dimensional example of this process.\n",
    "\n",
    "![Figure](simultaneous-diagonalization-example.png)\n",
    "\n",
    "\n",
    "The matrices $\\boldsymbol{\\Phi} \\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Psi}$ and $\\boldsymbol{\\Lambda}$ can be calculated directly from $\\boldsymbol{\\Sigma}_1$ and $\\boldsymbol{\\Sigma}_2$ without going through the two steps above as shown in the following theorem.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorem (p. 32)\n",
    "\n",
    "Let $\\boldsymbol{\\Sigma}_1$ and $\\boldsymbol{\\Sigma}_2$ be $p \\times p$ symmetric matrices.\n",
    "Then,\n",
    "\n",
    "$$\n",
    "\\textbf{A}' \\boldsymbol{\\Sigma}_1 \\textbf{A} = \\textbf{I}_p \\quad \\text{and} \\quad \\textbf{A}' \\boldsymbol{\\Sigma}_2 \\textbf{A} = \\boldsymbol{\\Lambda}\n",
    "$$\n",
    "\n",
    "are simultaneously diagonalized by $\\textbf{A}$, where $\\textbf{A}$ and $\\boldsymbol{\\Lambda}$ are the eigenvector and eigenvalue matrices of $\\boldsymbol{\\Sigma}_1^{-1} \\boldsymbol{\\Sigma}_2$, respectively, such that\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Sigma}_1^{-1} \\boldsymbol{\\Sigma}_2 \\textbf{A} = \\textbf{A} \\boldsymbol{\\Lambda}.\n",
    "$$\n",
    "\n",
    "#### Proof\n",
    "\n",
    "Because $\\textbf{K} \\boldsymbol{\\Psi} = \\boldsymbol{\\Psi} \\boldsymbol{\\Lambda}$, we know that the eigenvalues of $\\textbf{K}$ satisfy\n",
    "\n",
    "$$\n",
    "|\\textbf{K} - \\lambda \\textbf{I}_p| = 0.\n",
    "$$\n",
    "\n",
    "Hence, recalling that $\\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Phi}' \\boldsymbol{\\Sigma}_2 \\boldsymbol{\\Phi} \\boldsymbol{\\Theta}^{-1/2} = \\textbf{K}$ and $\\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Phi}' \\boldsymbol{\\Sigma}_1 \\boldsymbol{\\Phi} \\boldsymbol{\\Theta}^{-1/2} = \\textbf{I}_p$, we have\n",
    "\n",
    "\\begin{align}\n",
    "0\n",
    "    &= \\left|\\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Phi}' \\boldsymbol{\\Sigma}_2 \\boldsymbol{\\Phi} \\boldsymbol{\\Theta}^{-1/2} - \\lambda \\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Phi}' \\boldsymbol{\\Sigma}_1 \\boldsymbol{\\Phi} \\boldsymbol{\\Theta}^{-1/2} \\right| \\\\\n",
    "    &= \\left|\\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Phi}'(\\boldsymbol{\\Sigma}_2 - \\lambda \\boldsymbol{\\Sigma}_1 )\\boldsymbol{\\Phi} \\boldsymbol{\\Theta}^{-1/2}\\right| \\\\\n",
    "    &= \\left|\\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Phi}'\\right| \\left| \\boldsymbol{\\Sigma}_2 - \\lambda \\boldsymbol{\\Sigma}_1 \\right| \\left|\\boldsymbol{\\Phi} \\boldsymbol{\\Theta}^{-1/2}\\right|.\n",
    "\\end{align}\n",
    "\n",
    "Because $\\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Phi}'$ is nonsingular, $\\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Phi}' \\ne 0$. Hence, $\\left| \\boldsymbol{\\Sigma}_2 - \\lambda \\boldsymbol{\\Sigma}_1 \\right| = 0$, which implies $\\left| \\boldsymbol{\\Sigma}_1^{-1} \\boldsymbol{\\Sigma}_2 - \\lambda \\textbf{I}_p \\right| = 0$. Therefore, $\\boldsymbol{\\Lambda}$ is the eigenvalue matrix of $\\boldsymbol{\\Sigma}_1^{-1} \\boldsymbol{\\Sigma}_2$.\n",
    "\n",
    "Next, we show that $\\textbf{A} = \\boldsymbol{\\Phi} \\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Psi}$ is the eigenvector matrix of $\\boldsymbol{\\Sigma}_1^{-1} \\boldsymbol{\\Sigma}_2$. Substituting $\\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Phi}' \\boldsymbol{\\Sigma}_2 \\boldsymbol{\\Phi} \\boldsymbol{\\Theta}^{-1/2} = \\textbf{K}$ into $\\textbf{K} \\boldsymbol{\\Psi} = \\boldsymbol{\\Psi} \\boldsymbol{\\Lambda}$, we see that\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Phi}' \\boldsymbol{\\Sigma}_2 \\boldsymbol{\\Phi} \\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Psi} = \\boldsymbol{\\Psi} \\boldsymbol{\\Lambda},\n",
    "$$\n",
    "\n",
    "which implies\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Sigma}_2 \\boldsymbol{\\Phi} \\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Psi} = \\left( \\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Phi}' \\right)^{-1} \\boldsymbol{\\Psi} \\boldsymbol{\\Lambda}.\n",
    "$$\n",
    "\n",
    "Because $\\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Phi}' \\boldsymbol{\\Sigma}_1 \\boldsymbol{\\Phi} \\boldsymbol{\\Theta}^{-1/2} = \\textbf{I}_p$, it follows that\n",
    "\n",
    "$$\n",
    "\\left( \\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Phi}' \\right)^{-1} = \\boldsymbol{\\Sigma}_1 \\boldsymbol{\\Phi} \\boldsymbol{\\Theta}^{-1/2}.\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\Sigma}_2 \\boldsymbol{\\Phi} \\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Psi} &= \\boldsymbol{\\Sigma}_1 \\boldsymbol{\\Phi} \\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Psi} \\boldsymbol{\\Lambda} \\\\\n",
    "\\Rightarrow\n",
    "\\boldsymbol{\\Sigma}_1^{-1} \\boldsymbol{\\Sigma}_2 \\boldsymbol{\\Phi} \\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Psi} &= \\boldsymbol{\\Phi} \\boldsymbol{\\Theta}^{-1/2} \\boldsymbol{\\Psi} \\boldsymbol{\\Lambda} \\\\\n",
    "\\Rightarrow\n",
    "\\boldsymbol{\\Sigma}_1^{-1} \\boldsymbol{\\Sigma}_2 \\textbf{A} &= \\textbf{A} \\boldsymbol{\\Lambda}.\n",
    "\\end{align}\n",
    "\n",
    "$$\\tag*{$\\blacksquare$}$$\n",
    "\n",
    "It is important to note that because $\\boldsymbol{\\Sigma}_1^{-1} \\boldsymbol{\\Sigma}_2$ is not symmetric in general, and subsequently its eigenvectors $\\boldsymbol{\\psi}_j$ are not mutually orthogonal, i.e., $\\boldsymbol{\\psi}_i'\\boldsymbol{\\psi}_j = 0$ for $i \\ne j$. Instead, the $\\boldsymbol{\\psi}_j$'s are orthogonal with respect to $\\boldsymbol{\\Sigma}_1$ such that $\\boldsymbol{\\psi}_i' \\boldsymbol{\\Sigma}_1 \\boldsymbol{\\psi}_j = 0$ for $i \\ne j$. Furthermore, in order to make the $\\boldsymbol{\\psi}_j$'s are orthonormal with respect to $\\boldsymbol{\\Sigma}_1$ to satisfy $\\textbf{A}' \\boldsymbol{\\Sigma}_1 \\textbf{A} = \\textbf{I}_p$, the scale of $\\boldsymbol{\\psi}_j$ must be adjusted by $\\boldsymbol{\\psi}_j' \\boldsymbol{\\Sigma}_1 \\boldsymbol{\\psi}_j$ such that\n",
    "\n",
    "$$\n",
    "\\dfrac{\\boldsymbol{\\psi}_j'}{\\sqrt{\\boldsymbol{\\psi}_j' \\boldsymbol{\\Sigma}_1 \\boldsymbol{\\psi}_j}} \\boldsymbol{\\Sigma}_1 \\dfrac{\\boldsymbol{\\psi}_j}{\\sqrt{\\boldsymbol{\\psi}_j' \\boldsymbol{\\Sigma}_1 \\boldsymbol{\\psi}_j}} = 1.\n",
    "$$\n",
    "\n",
    "> Simultaneous diagonalization of two matrices is a very powerful tool in pattern recognition because many problems of pattern recognition consider two distributions for classification purposes. Also, there are many possible modifications of the above discussion. These depend on what kind of properties we are interested in, what kind of matrices are used, etc. In this section we will show one of the modifications that will be used in later chapters.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorem (p. 33)\n",
    "\n",
    "Let a matrix $\\textbf{Q}$ be given by a linear combination of two symmetric matrices $\\textbf{Q}_1$ and $\\textbf{Q}_2$ as\n",
    "\n",
    "$$\n",
    "\\textbf{Q} = a_1 \\textbf{Q}_1 + a_2 \\textbf{Q}_2,\n",
    "$$\n",
    "\n",
    "where $a_1, a_2 > 0$. If we normalize the eigenvectors with respect to $\\textbf{Q}$ to satisfy $\\textbf{A}' \\boldsymbol{\\Sigma}_1 \\textbf{A} = \\textbf{I}_p$ above, $\\textbf{Q}_1$ and $\\textbf{Q}_2$ will share the same eigenvectors, and their eigenvalues will be reversely ordered as\n",
    "\n",
    "\\begin{align}\n",
    "\\lambda_1^{(1)} > \\lambda_2^{(1)} > \\ldots > \\lambda_n^{(1)} &\\text{ for } \\textbf{Q}_1, \\\\\n",
    "\\lambda_1^{(2)} < \\lambda_2^{(2)} < \\ldots < \\lambda_n^{(2)} &\\text{ for } \\textbf{Q}_2.\n",
    "\\end{align}\n",
    "\n",
    "#### Proof\n",
    "\n",
    "Let $\\textbf{Q}$ and $\\textbf{Q}_1$ be diagonalized simultaneously such that\n",
    "\n",
    "$$\n",
    "\\textbf{A}' \\textbf{Q} \\textbf{A} = \\textbf{I}_p \\quad \\text{and} \\quad \\textbf{A}' \\textbf{Q}_1 \\textbf{A} = \\boldsymbol{\\Lambda}^{(1)},\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\textbf{Q}^{-1} \\textbf{Q}_1 \\textbf{A} = \\textbf{A} \\boldsymbol{\\Lambda}^{(1)}.\n",
    "$$\n",
    "\n",
    "Then $\\textbf{Q}_2$ is also diagonalized because\n",
    "\n",
    "\\begin{align}\n",
    "    \\textbf{I}_p\n",
    "    &= \\textbf{A}' \\textbf{Q} \\textbf{A} \\\\\n",
    "    &= \\textbf{A}' (a_1 \\textbf{Q}_1 + a_2 \\textbf{Q}_2) \\textbf{A} \\\\\n",
    "    &= a_1 \\textbf{A}' \\textbf{Q}_1 \\textbf{A} + a_2 \\textbf{A}' \\textbf{Q}_2 \\textbf{A} \\\\\n",
    "    &= a_1 \\boldsymbol{\\Lambda}^{(1)} + a_2 \\textbf{A}' \\textbf{Q}_2 \\textbf{A},\n",
    "\\end{align}\n",
    "\n",
    "which implies that\n",
    "\n",
    "$$\n",
    "\\textbf{A}' \\textbf{Q}_2 \\textbf{A} = \\dfrac{1}{a_2} \\left(\\textbf{I}_p - a_1 \\boldsymbol{\\Lambda}^{(1)} \\right).\n",
    "$$\n",
    "\n",
    "That is,\n",
    "\n",
    "$$\n",
    "\\lambda_j^{(2)} = \\dfrac{1 - a_1 \\lambda_j^{(1)}}{a_2},\n",
    "$$\n",
    "\n",
    "which implies that, if $\\lambda_i^{(1)} > \\lambda_j^{(1)}$, then $\\lambda_i^{(2)} < \\lambda_j^{(2)}$. Furthermore, $\\textbf{Q}_1$ and $\\textbf{Q}_2$ share the same eigenvectors that are normalized with respect to $\\textbf{Q}$.\n",
    "\n",
    "$$\\tag*{$\\blacksquare$}$$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fukunaga defines the autocorrelation matrix $\\textbf{S}$ and the covariance matrix $\\boldsymbol{\\Sigma}$ as\n",
    "\n",
    "\\begin{align}\n",
    "\\textbf{S} &= E[\\textbf{XX}'], \\\\\n",
    "\\boldsymbol{\\Sigma} &= \\textbf{S} - \\textbf{mm}',\n",
    "\\end{align}\n",
    "\n",
    "where $\\textbf{m} = E[\\textbf{X}]$.\n",
    "\n",
    "The following example uses the Theorem above.\n",
    "\n",
    "### Example (p. 34)\n",
    "\n",
    "Let $\\textbf{S}$ be the mixture autocorrelation matrix of two distributions who autocorrelation matrices are $\\textbf{S}_1$ and $\\textbf{S}_2$. Then\n",
    "\n",
    "\\begin{align}\n",
    "    \\textbf{S}\n",
    "    &= E[\\textbf{XX}'] \\\\\n",
    "    &= P_1 E[\\textbf{XX}' | \\omega_1] + P_2 E[\\textbf{XX}' | \\omega_2] \\\\\n",
    "    &= P_1 \\textbf{S}_1 + P_2 \\textbf{S}_2.\n",
    "\\end{align}\n",
    "\n",
    "> Thus, by the above theorem, we can diagonalize $\\textbf{S}_1$ and $\\textbf{S}_2$ with the same set of eigenvectors. Since the eigenvalues are ordered in reverse, the eigenvector with the largest eigenvalue for the first distribution has the least eigenvalue for the second, and vice versa. **This property can be used to extract features important to distinguish two distributions.**\n",
    "\n",
    "This important finding is the so-called [Fukunagaâ€“Koontz Transform](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=1671511). Here are a couple of examples that cite this transform.\n",
    "\n",
    "* [Paper #1](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7044575)\n",
    "* [Paper #2](https://users.ece.cmu.edu/~juefeix/felix_pr16_fkda.pdf)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between $|\\textbf{S}|$ and $|\\boldsymbol{\\Sigma}|$ (p. 38)\n",
    "\n",
    "Simultaneous diagonalization enables us to establish the relationship between the determinants of the autocorrelation matrix $\\textbf{S}$ and the covariance matrix $\\boldsymbol{\\Sigma}$.\n",
    "\n",
    "Note that $\\textbf{S} = \\boldsymbol{\\Sigma} + \\textbf{mm}'$. Applying the simultaneous diagonalization of $\\textbf{A}' \\boldsymbol{\\Sigma}_1 \\textbf{A} = \\textbf{I}_p$ and $\\textbf{A}' \\boldsymbol{\\Sigma}_2 \\textbf{A} = \\boldsymbol{\\Lambda}$ with $\\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}$ and $\\boldsymbol{\\Sigma}_2 = \\textbf{mm}'$, we have $\\textbf{A}'(\\boldsymbol{\\Sigma} + \\textbf{mm}')\\textbf{A} = \\textbf{I}_p + \\boldsymbol{\\Lambda}$. Notice that $|\\textbf{A}'| |\\boldsymbol{\\Sigma}| |\\textbf{A}| = |\\textbf{I}_p|$, which implies that $|\\boldsymbol{\\Sigma}| = 1 / |\\textbf{A}|^2$. Therefore, $|\\textbf{A}'| |\\boldsymbol{\\Sigma} + \\textbf{mm}'| |\\textbf{A}| = |\\textbf{I}_p + \\boldsymbol{\\Lambda}|$, which implies\n",
    "\n",
    "\\begin{align}\n",
    "    |\\boldsymbol{\\Sigma} + \\textbf{mm}'| &= \\dfrac{|\\textbf{I}_p + \\boldsymbol{\\Lambda}|}{|\\textbf{A}|^2} \\\\\n",
    "    &= |\\textbf{I}_p + \\boldsymbol{\\Lambda}| |\\boldsymbol{\\Sigma}| \\\\\n",
    "    &= |\\boldsymbol{\\Sigma}| \\prod_{j=1}^p (1 + \\lambda_j).\n",
    "\\end{align}\n",
    "\n",
    "Notice that rank$(\\textbf{mm}') = 1$ if $\\textbf{m} \\ne \\textbf{0}$ so that\n",
    "\n",
    "$$\n",
    "\\lambda_1 \\ne 0, \\quad \\lambda_2 = \\ldots = \\lambda_p = 0,\n",
    "$$\n",
    "\n",
    "implying that $|\\boldsymbol{\\Sigma} + \\textbf{mm}'| = |\\boldsymbol{\\Sigma}| (1 + \\lambda_1)$. Also, notice that, if $\\textbf{A}$ is nonsingular, $\\boldsymbol{\\Sigma}^{-1} = \\textbf{AA'}$. Hence,\n",
    "\n",
    "\\begin{align}\n",
    "    \\lambda_1\n",
    "    &= \\sum_{j=1}^p \\lambda_j \\\\\n",
    "    &= \\text{tr}\\{\\boldsymbol{\\Lambda}\\} \\\\\n",
    "    &= \\text{tr}\\{ \\textbf{A}'\\textbf{mm}'\\textbf{A} \\} \\\\\n",
    "    &= \\text{tr}\\{ \\textbf{m}'\\textbf{AA}'\\textbf{m} \\} \\\\\n",
    "    &= \\text{tr}\\{ \\textbf{m}'\\boldsymbol{\\Sigma}^{-1}\\textbf{m} \\} \\\\\n",
    "    &= \\textbf{m}'\\boldsymbol{\\Sigma}^{-1}\\textbf{m}.\n",
    "\\end{align}\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "    |\\boldsymbol{\\Sigma} + \\textbf{mm}'| = |\\boldsymbol{\\Sigma}| (1 + \\textbf{m}'\\boldsymbol{\\Sigma}^{-1}\\textbf{m}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between $\\textbf{S}^{-1}$ and $\\boldsymbol{\\Sigma}^{-1}$ (p. 42)\n",
    "\n",
    "Similar to above, simultaneous diagonalization enables us to establish the relationship between $\\textbf{S}^{-1}$ and $\\boldsymbol{\\Sigma}^{-1}$.\n",
    "\n",
    "Recall that $\\textbf{A}'(\\boldsymbol{\\Sigma} + \\textbf{mm}')\\textbf{A} = \\textbf{I}_p + \\boldsymbol{\\Lambda}$. Because $\\textbf{A}$ is nonsingular, $\\boldsymbol{\\Sigma} + \\textbf{mm}' = (\\textbf{A}')^{-1}(\\textbf{I}_p + \\boldsymbol{\\Lambda})\\textbf{A}^{-1}$, which implies\n",
    "\n",
    "$$\n",
    "(\\boldsymbol{\\Sigma} + \\textbf{mm}')^{-1} = \\textbf{A}(\\textbf{I}_p + \\boldsymbol{\\Lambda})^{-1}\\textbf{A}'.\n",
    "$$\n",
    "\n",
    "Using the above example, we have that\n",
    "\n",
    "\\begin{align}\n",
    "    (\\textbf{I}_p + \\boldsymbol{\\Lambda})^{-1}\n",
    "    &= \\text{diag}\\left(\\frac{1}{1 + \\lambda_1}, 1, \\ldots, 1 \\right) \\\\\n",
    "    &= \\text{diag}\\left(1 - \\frac{\\lambda_1}{1 + \\lambda_1}, 1, \\ldots, 1 \\right) \\\\\n",
    "    &= \\textbf{I}_p - \\dfrac{1}{1 + \\lambda_1} \\boldsymbol{\\Lambda}.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Because $\\textbf{A}'\\textbf{mm}'\\textbf{A} = \\boldsymbol{\\Lambda}$ from the simultaneous diagonalization, it follows that\n",
    "\n",
    "$$\n",
    "    \\textbf{A}\\boldsymbol{\\Lambda}\\textbf{A}'\n",
    "    = \\textbf{AA}'\\textbf{mm}'\\textbf{AA}'\n",
    "    = \\boldsymbol{\\Sigma}^{-1}\\textbf{mm}'\\boldsymbol{\\Sigma}^{-1},\n",
    "$$\n",
    "\n",
    "where the second equality follows recalling that $\\boldsymbol{\\Sigma}^{-1} = \\textbf{AA}'$. Again, using the above example, we have that\n",
    "\n",
    "\\begin{align}\n",
    "    \\textbf{S}^{-1}\n",
    "    &= \\textbf{A}\\left( \\textbf{I}_p - \\dfrac{1}{1 + \\lambda_1} \\boldsymbol{\\Lambda} \\right)\\textbf{A}' \\\\\n",
    "    &= \\textbf{AA}' - \\dfrac{1}{1 + \\lambda_1} \\textbf{A}\\boldsymbol{\\Lambda}\\textbf{A'} \\\\\n",
    "    &= \\boldsymbol{\\Sigma}^{-1} - \\dfrac{1}{1 + \\lambda_1} \\boldsymbol{\\Sigma}^{-1} \\textbf{mm}'\\boldsymbol{\\Sigma}^{-1} \\\\\n",
    "    &= \\boldsymbol{\\Sigma}^{-1} - \\dfrac{1}{1 + \\textbf{m}'\\boldsymbol{\\Sigma}^{-1}\\textbf{m}} \\boldsymbol{\\Sigma}^{-1} \\textbf{mm}'\\boldsymbol{\\Sigma}^{-1} \\quad (\\text{recall: } \\lambda_1 = \\textbf{m}'\\boldsymbol{\\Sigma}^{-1}\\textbf{m}).\n",
    "\\end{align}\n",
    "\n",
    "If we would further like to calculate the quadratic form $\\textbf{m}'\\textbf{S}^{-1}\\textbf{m}$ in terms of $\\textbf{m}'\\boldsymbol{\\Sigma}^{-1}\\textbf{m}$, then\n",
    "\n",
    "\\begin{align}\n",
    "    \\textbf{m}'\\textbf{S}^{-1}\\textbf{m}\n",
    "    &= \\textbf{m}'\\boldsymbol{\\Sigma}^{-1}\\textbf{m} - \\dfrac{1}{1 + \\textbf{m}'\\boldsymbol{\\Sigma}^{-1}\\textbf{m}}  (\\textbf{m}'\\boldsymbol{\\Sigma}^{-1}\\textbf{m})^2 \\\\\n",
    "    &= \\dfrac{\\textbf{m}'\\boldsymbol{\\Sigma}^{-1}\\textbf{m}}{1 + \\textbf{m}'\\boldsymbol{\\Sigma}^{-1}\\textbf{m}}.\n",
    "\\end{align}\n",
    "\n",
    "Similarly,\n",
    "\n",
    "$$\n",
    "    \\textbf{m}'\\boldsymbol{\\Sigma}^{-1}\\textbf{m} = \\dfrac{\\textbf{m}'\\textbf{S}^{-1}\\textbf{m}}{1 - \\textbf{m}'\\textbf{S}^{-1}\\textbf{m}}.\n",
    "$$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Inversion (p. 41-42)\n",
    "\n",
    "Simultaneous diagonalization can yield a significant reduction in computation by preprocessing the data when computing a distance function involves a matrix inverse. For two distributions, the distance functions are, by simultaneous diagonalization,\n",
    "\n",
    "\\begin{align}\n",
    "    d_1(\\textbf{x})\n",
    "    &= (\\textbf{x} - \\textbf{m}_1)'\\boldsymbol{\\Sigma}_1^{-1}(\\textbf{x} - \\textbf{m}_1) \\\\\n",
    "    &= (\\textbf{y} - \\textbf{d}_1)'\\textbf{I}_p{-1}(\\textbf{y} - \\textbf{d}_1) \\\\\n",
    "    &= \\sum_{j=1}^p (y_j - d_{1j})^2, \\\\\n",
    "    d_2(\\textbf{x})\n",
    "    &= (\\textbf{x} - \\textbf{m}_2)'\\boldsymbol{\\Sigma}_2^{-1}(\\textbf{x} - \\textbf{m}_2) \\\\\n",
    "    &= (\\textbf{y} - \\textbf{d}_2)'\\boldsymbol{\\Lambda}^{-1}(\\textbf{y} - \\textbf{d}_2) \\\\\n",
    "    &= \\sum_{j=1}^p \\dfrac{(y_j - d_{2j})^2}{\\lambda_j},\n",
    "\\end{align}\n",
    "\n",
    "where $\\textbf{y} = \\textbf{A}'\\textbf{x}$ and $\\textbf{d}_{k} = \\textbf{A}'\\textbf{m}_k$, $k = 1, 2$.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimum Linear Transformation (p. 448)\n",
    "\n",
    "Feature extraction for classification consists of choosing those features which are most effective for preserving class separability. We can consider feature extraction for classification as a search, among all posssible singular transformations, for the best subspace which preserves class separability as much as possible in the lowest possible dimensional space.\n",
    "\n",
    "A linear transformation from a $p$-dimensional $\\textbf{x}$ to an $q$-dimensional $\\textbf{y}$ $(q < p)$ is expressed by\n",
    "\n",
    "$$\n",
    "\\textbf{y} = \\textbf{A}'\\textbf{x},\n",
    "$$\n",
    "\n",
    "where $\\textbf{A}$ is a $p \\times q$ rectangular matrix and the column vectors are linearly independent. These column vectors do not need to be orthonormal. Moreover, because $q < p$, $\\textbf{A}$ is singular and can yield a low-dimensional projection.\n",
    "\n",
    "The within-class $\\textbf{S}_w$, between-class $\\textbf{S}_b$, and mixture (total) $\\textbf{S}_m$ scatter matrices are used to formulate criteria of class separability. The within-class scatter matrix $\\textbf{S}_w$ shows the scatter of samples around their respective class expected vectors. On the other hand, the between-class scatter matrix $\\textbf{S}_b$ is the scatter of the expected vectors around the mixture (grand) mean. The mixture (total) scatter matrix $\\textbf{S}_m$ is the covariance matrix of all samples regardless of their class assignments such that\n",
    "\n",
    "$$\n",
    "\\textbf{S}_m = \\textbf{S}_b + \\textbf{S}_w.\n",
    "$$\n",
    "\n",
    "Fukunaga uses notation that can be confusing at times to interchange any of these three matrices when defining class separability. That is, let $\\textbf{S}_1$ and $\\textbf{S}_2$ be one of $\\textbf{S}_m$, $\\textbf{S}_b$, and $\\textbf{S}_w$.\n",
    "\n",
    "> In order to formulate criteria for class separability, we need to convert these matrices to a number. This number should be larger when the between-class scatter is larger or the within-class scatter is smaller. There are several ways to do this:\n",
    "\n",
    "1. $J_1 = \\text{tr }(\\textbf{S}_2^{-1} \\textbf{S}_1)$.\n",
    "2. $J_2 = \\ln|\\textbf{S}_2^{-1} \\textbf{S}_1| = \\ln|\\textbf{S}_1| - \\ln|\\textbf{S}_2|$.\n",
    "3. $J_3 = \\text{tr }\\textbf{S}_1 - \\mu( \\text{tr }\\textbf{S}_2 - c)$.\n",
    "4. $J_4 = \\dfrac{\\text{tr }\\textbf{S}_1}{\\text{tr }\\textbf{S}_2}$.\n",
    "\n",
    "Many combinations of $\\textbf{S}_m$, $\\textbf{S}_b$, and $\\textbf{S}_w$ for $\\textbf{S}_1$ and $\\textbf{S}_2$ are possible. Typical examples for $\\{\\textbf{S}_1, \\textbf{S}_2\\}$ are\n",
    "\n",
    "* $\\{\\textbf{S}_b, \\textbf{S}_w\\}$\n",
    "* $\\{\\textbf{S}_b, \\textbf{S}_m\\}$\n",
    "* $\\{\\textbf{S}_w, \\textbf{S}_m\\}$.\n",
    "\n",
    "Fukunaga further distinguishes $\\textbf{S}_1$ and $\\textbf{S}_2$ as covariance matrices in the $\\textbf{X}$-space with covariance matrices in the $\\textbf{Y}$-space by\n",
    "\n",
    "$$\n",
    "\\textbf{S}_{iY} = \\textbf{A}'\\textbf{S}_{iX}\\textbf{A}.\n",
    "$$\n",
    "\n",
    "Thus, the problem of feature extraction for classification is to find the $\\textbf{A}$ which optimizes one of the $J$s in the $\\textbf{Y}$-space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization of $J_1$\n",
    "\n",
    "Let $J_1(m)$ be the value of $J_1$ in an $m$-dimensional $\\textbf{Y}$-space. Then\n",
    "\n",
    "$$\n",
    "J_1(m) = \\text{tr }(\\textbf{S}_{2Y}^{-1} \\textbf{S}_{1Y}) = \\text{tr }\\{(\\textbf{A}'\\textbf{S}_{2Y}\\textbf{A})^{-1} \\textbf{A}'\\textbf{S}_{1Y}\\textbf{A}\\}.\n",
    "$$\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
